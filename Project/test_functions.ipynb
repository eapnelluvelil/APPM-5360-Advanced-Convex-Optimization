{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100\n",
    "device = \"cuda\"\n",
    "\n",
    "def synthetic_example(iters=100_000, lr=1e-3):\n",
    "    # Objective function\n",
    "    def func(x):\n",
    "        val = 0\n",
    "        for i in np.arange(d - 1):\n",
    "            val += (100*(x[i + 1] - x[i]**2)**2 + (x[i] - 1)**2)\n",
    "        return val\n",
    "    \n",
    "    x0 = np.random.uniform(-2.048, 2.048, d)\n",
    "\n",
    "    print(\"Initial guess: \")\n",
    "    print(x0)\n",
    "\n",
    "    x_Adam = Variable(torch.tensor(x0), requires_grad=True)\n",
    "    x_AMS  = Variable(torch.tensor(x0), requires_grad=True)\n",
    "    x_SGD  = Variable(torch.tensor(x0), requires_grad=True)\n",
    "\n",
    "    # avg_regret_checkpoints = []\n",
    "    # iteration_checkpoints  = []\n",
    "    # x_checkpoints          = []\n",
    "\n",
    "    optimizer_Adam = torch.optim.Adam([x_Adam], lr=lr, betas=(0.9, 0.999), eps=1e-08, amsgrad=False)\n",
    "    optimizer_AMSGrad = torch.optim.Adam([x_AMS], lr=lr, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "    optimizer_SGD  = torch.optim.SGD([x_SGD], lr=lr, momentum=0.9, dampening=0,\n",
    "                                     weight_decay=0, nesterov=True)\n",
    "    \n",
    "    # Create learning rate schedulers for Adam and AMSGrad\n",
    "    lambda1 = lambda iter: 1/np.sqrt(iter + 1)\n",
    "    scheduler_Adam = torch.optim.lr_scheduler.LambdaLR(optimizer_Adam, lr_lambda=lambda1,\n",
    "                                                       verbose=False)\n",
    "\n",
    "    scheduler_AMS  = torch.optim.lr_scheduler.LambdaLR(optimizer_AMSGrad, lr_lambda=lambda1,\n",
    "                                                       verbose=False)\n",
    "\n",
    "    # lambda3 = lambda iter: 1/np.sqrt(iter + 1)\n",
    "    # scheduler_SGD  = torch.optim.lr_scheduler.LambdaLR(optimizer_SGD, lr_lambda=lambda3,\n",
    "                                                    #    verbose=False)\n",
    "\n",
    "    # total_regret = 0\n",
    "\n",
    "    for iter in np.arange(1, iters + 1):\n",
    "        loss_Adam     = func(x_Adam)\n",
    "        loss_AMS      = func(x_AMS)\n",
    "        loss_SGD      = func(x_SGD)\n",
    "\n",
    "        # total_regret += np.linalg.norm(loss.item() - x_true)\n",
    "\n",
    "        # if (iter % 10000 == 0):\n",
    "        #     avg_regret = total_regret / iter\n",
    "        #     avg_regret_checkpoints.append(avg_regret)\n",
    "        #     iteration_checkpoints.append(iter)\n",
    "        #     x_checkpoints.append(x.item())\n",
    "\n",
    "        if (iter % 10000 == 0):\n",
    "            print(f\"Iteration: {iter}\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"f(x_Adam) = {func(x_Adam)}\")\n",
    "            print(f\"f(x_AMS)  = {func(x_AMS)}\")\n",
    "            print()\n",
    "\n",
    "        optimizer_Adam.zero_grad()\n",
    "        loss_Adam.backward()\n",
    "        optimizer_Adam.step()\n",
    "        scheduler_Adam.step()\n",
    "\n",
    "        optimizer_AMSGrad.zero_grad()\n",
    "        loss_AMS.backward()\n",
    "        optimizer_AMSGrad.step()\n",
    "        scheduler_AMS.step()\n",
    "\n",
    "        # optimizer_SGD.zero_grad()\n",
    "        # loss_SGD.backward()\n",
    "        # optimizer_SGD.step()\n",
    "        # scheduler_SGD.step()\n",
    "\n",
    "    # return x_Adam, x_AMS, x_SGD\n",
    "    return x_Adam, x_AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial guess: \n",
      "[-1.21543448 -1.83352503  1.88962754  0.38736266 -0.63890009  1.67301002\n",
      "  1.43122575  0.9633221  -1.53772435  1.69497534 -0.07942481  1.24619746\n",
      " -0.06625525  0.70825886 -1.72496726 -1.05068075  0.71528666 -0.78439539\n",
      " -1.13651196  0.95651966  0.49545871  1.47285548  0.82852624 -1.94276572\n",
      "  0.68108261 -0.93660826  0.18271864  1.1647551  -1.79214006 -1.96330496\n",
      " -1.90019148 -1.0489831  -0.19136372 -1.83809797 -1.28275548 -0.12809114\n",
      "  1.68608313  0.43220145  1.18236565  0.50168503 -0.93107045 -1.61207924\n",
      " -0.46878759  0.97444819  1.62343849 -1.46974484  1.67211728  0.58763585\n",
      "  0.91446441 -1.98483777 -1.35799843 -0.02987472  0.73625436  0.97984658\n",
      " -1.07628695 -1.56495946 -0.20759028 -0.01199815 -0.31392419  1.27812989\n",
      " -1.19890956  0.36165421  0.48992165 -1.48924547  0.44915513  0.73184001\n",
      " -1.11694419  0.72723694  0.19307259 -0.92100698  0.49966358  0.78266792\n",
      "  1.81341711  1.53661033 -1.03762018  1.14328076  0.15958244 -1.93770351\n",
      " -1.59791338 -0.12430738 -0.24787699  0.77777484 -0.99146846  1.104376\n",
      "  0.25887304  0.47371238 -0.92022003  0.30911986  1.84702854  1.65192121\n",
      "  1.44763345 -0.49062092 -2.0248552   0.39535703 -1.93638844  0.84269308\n",
      " -1.59448971 -0.20210931  0.14912265 -0.86277326]\n",
      "Iteration: 10000\n",
      "---------------------------\n",
      "f(x_Adam) = 29470.943837771898\n",
      "f(x_AMS)  = 29940.81992129416\n",
      "\n",
      "Iteration: 20000\n",
      "---------------------------\n",
      "f(x_Adam) = 23817.74852347838\n",
      "f(x_AMS)  = 25260.381773469002\n",
      "\n",
      "Iteration: 30000\n",
      "---------------------------\n",
      "f(x_Adam) = 20138.21580825596\n",
      "f(x_AMS)  = 22415.920320875393\n",
      "\n",
      "Iteration: 40000\n",
      "---------------------------\n",
      "f(x_Adam) = 17435.393539099383\n",
      "f(x_AMS)  = 20397.101349910274\n",
      "\n",
      "Iteration: 50000\n",
      "---------------------------\n",
      "f(x_Adam) = 15321.25297564586\n",
      "f(x_AMS)  = 18850.004315432616\n",
      "\n",
      "2-norm between Adam x and true x:    12.958039138640341\n",
      "2-norm between AMSGrad x and true x: 13.39985289696798\n"
     ]
    }
   ],
   "source": [
    "x_true = torch.tensor(np.ones(d))\n",
    "\n",
    "iters = 50000\n",
    "lr    = 1e-3\n",
    "# x_Adam, x_AMS, x_SGD = synthetic_example(iters=iters, lr=lr)\n",
    "x_Adam, x_AMS = synthetic_example(iters=iters, lr=lr)\n",
    "\n",
    "print(f\"2-norm between Adam x and true x:    {torch.linalg.vector_norm(x_Adam - x_true)}\")\n",
    "print(f\"2-norm between AMSGrad x and true x: {torch.linalg.vector_norm(x_AMS - x_true)}\")\n",
    "# print(f\"2-norm between SGD x and true x:     {torch.linalg.vector_norm(x_SGD - x_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinity-norm between Adam x and true x:    2.9466385363488605\n",
      "Infinity-norm between AMSGrad x and true x: 2.946638536260939\n",
      "\n",
      "tensor([-1.6279,  0.9314,  1.3776, -1.8301,  1.5839,  1.9376,  0.5168,  0.5733,\n",
      "        -1.5466, -0.1653, -1.3776,  0.5230, -1.0613, -1.0587, -0.2595,  1.8116,\n",
      "        -1.0637, -0.3622,  0.4930,  1.4301,  0.1768,  0.5528, -1.2596,  1.4856,\n",
      "         0.2897,  0.0982,  0.4462,  1.8783,  1.4280, -1.8431, -1.0537, -1.6630,\n",
      "         0.1492, -0.9179, -0.0432,  0.7669, -0.8113, -0.0878, -0.1132,  1.7207,\n",
      "        -0.9251, -1.7720, -1.1809,  0.7866,  0.0638,  1.1108, -1.1978,  1.1405,\n",
      "        -0.0583, -1.8863,  1.5367, -1.3935,  0.3069, -0.3668, -1.7436, -0.3047,\n",
      "         1.9601, -1.8675, -0.8760,  1.6220,  0.1330,  0.5641, -0.7270,  0.5134,\n",
      "        -1.2977,  0.5716, -0.2925, -1.2829, -1.0185, -1.1671, -1.2500, -1.9312,\n",
      "         0.6983, -1.6910,  1.1969,  1.4270, -0.4979, -1.3388, -1.8272,  1.5727,\n",
      "         1.9107, -1.9466,  1.7700, -1.9163, -1.4286, -0.0072,  1.4610,  1.3333,\n",
      "        -1.5928,  0.6571, -1.6458,  1.4750,  0.8661, -0.3570,  0.6478,  1.9158,\n",
      "         0.2637,  0.5795,  0.5346,  1.5096], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "\n",
      "tensor([-1.6279,  0.9314,  1.3776, -1.8301,  1.5848,  1.9376,  0.5168,  0.5733,\n",
      "        -1.5466, -0.1653, -1.3776,  0.5233, -1.0613, -1.0587, -0.2595,  1.8116,\n",
      "        -1.0637, -0.3622,  0.4930,  1.4301,  0.1768,  0.5528, -1.2596,  1.4856,\n",
      "         0.2897,  0.0982,  0.4462,  1.8783,  1.4280, -1.8431, -1.0537, -1.6630,\n",
      "         0.1492, -0.9179, -0.0432,  0.7669, -0.8113, -0.0878, -0.1132,  1.7207,\n",
      "        -0.9251, -1.7720, -1.1809,  0.7866,  0.0638,  1.1108, -1.1978,  1.1405,\n",
      "        -0.0583, -1.8863,  1.5367, -1.3935,  0.3069, -0.3668, -1.7436, -0.3047,\n",
      "         1.9601, -1.8675, -0.8760,  1.6220,  0.1330,  0.5641, -0.7270,  0.5134,\n",
      "        -1.2977,  0.5716, -0.2925, -1.2829, -1.0185, -1.1671, -1.2500, -1.9312,\n",
      "         0.6986, -1.6910,  1.1969,  1.4270, -0.4979, -1.3388, -1.8272,  1.5728,\n",
      "         1.9107, -1.9466,  1.7700, -1.9163, -1.4286, -0.0072,  1.4610,  1.3333,\n",
      "        -1.5928,  0.6571, -1.6458,  1.4750,  0.8661, -0.3570,  0.6478,  1.9158,\n",
      "         0.2637,  0.5795,  0.5346,  1.5096], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Infinity-norm between Adam x and true x:    {torch.linalg.vector_norm(x_Adam - x_true, float('inf'))}\")\n",
    "print(f\"Infinity-norm between AMSGrad x and true x: {torch.linalg.vector_norm(x_AMS - x_true, float('inf'))}\")\n",
    "print()\n",
    "# print(f\"Infinity-norm between SGD x and true x:     {torch.linalg.vector_norm(x_SGD - x_true, float('inf'))}\")\n",
    "\n",
    "print(x_Adam)\n",
    "print()\n",
    "print(x_AMS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvxpy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
